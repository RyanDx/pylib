{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load test_util_tf.py\n",
    "from __future__ import print_function\n",
    "\n",
    "\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.python.ops import nn\n",
    "    relu = nn.relu\n",
    "    slim = tf.contrib.slim\n",
    "    sigmoid = nn.sigmoid\n",
    "    softmax = nn.softmax\n",
    "except:\n",
    "    print(\"tensorflow is not installed, util.tf can not be used.\")\n",
    "\n",
    "def is_gpu_available(cuda_only=True):\n",
    "  \"\"\"\n",
    "  code from https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/platform/test.py\n",
    "  Returns whether TensorFlow can access a GPU.\n",
    "  Args:\n",
    "    cuda_only: limit the search to CUDA gpus.\n",
    "  Returns:\n",
    "    True iff a gpu device of the requested kind is available.\n",
    "  \"\"\"\n",
    "  from tensorflow.python.client import device_lib as _device_lib\n",
    "\n",
    "  if cuda_only:\n",
    "    return any((x.device_type == 'GPU')\n",
    "               for x in _device_lib.list_local_devices())\n",
    "  else:\n",
    "    return any((x.device_type == 'GPU' or x.device_type == 'SYCL')\n",
    "               for x in _device_lib.list_local_devices())\n",
    "\n",
    "\n",
    "\n",
    "def get_available_gpus(num_gpus = None):\n",
    "    \"\"\"\n",
    "    Modified on http://stackoverflow.com/questions/38559755/how-to-get-current-available-gpus-in-tensorflow\n",
    "    However, the original code will occupy all available gpu memory.\n",
    "    The modified code need a parameter: num_gpus. It does nothing but return the device handler name\n",
    "    It will work well on single-maching-training, but I don't know whether it will work well on a cluster.\n",
    "    \"\"\"\n",
    "    if num_gpus == None:\n",
    "        from tensorflow.python.client import device_lib as _device_lib\n",
    "        local_device_protos = _device_lib.list_local_devices()\n",
    "        return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "    else:\n",
    "        return ['/gpu:%d'%(idx) for idx in xrange(num_gpus)]\n",
    "\n",
    "def get_latest_ckpt(path):\n",
    "# tf.train.latest_checkpoint\n",
    "    import util\n",
    "    path = util.io.get_absolute_path(path)\n",
    "    if util.io.is_dir(path):\n",
    "        ckpt = tf.train.get_checkpoint_state(path)\n",
    "        if ckpt is not None:\n",
    "            ckpt_path = ckpt.model_checkpoint_path\n",
    "        else:\n",
    "            ckpt_path = None\n",
    "    else:\n",
    "        ckpt_path = path;\n",
    "    return ckpt_path\n",
    "\n",
    "def get_all_ckpts(path):\n",
    "    ckpt = tf.train.get_checkpoint_state(path)\n",
    "    all_ckpts = ckpt.all_model_checkpoint_paths\n",
    "    ckpts = [str(c) for c in all_ckpts]\n",
    "    return ckpts\n",
    "\n",
    "def get_iter(ckpt):\n",
    "    import util\n",
    "    iter_ = int(util.str.find_all(ckpt, '.ckpt-\\d+')[0].split('-')[-1])\n",
    "    return iter_\n",
    "\n",
    "def get_init_fn(checkpoint_path, train_dir, ignore_missing_vars = False,\n",
    "                checkpoint_exclude_scopes = None, model_name = None, checkpoint_model_scope = None):\n",
    "    \"\"\"\n",
    "    code from github/SSD-tensorflow/tf_utils.py\n",
    "    Returns a function run by the chief worker to warm-start the training.\n",
    "    Note that the init_fn is only run when initializing the model during the very\n",
    "    first global step.\n",
    "\n",
    "    checkpoint_path: the checkpoint to be restored\n",
    "    train_dir: the directory where checkpoints are stored during training.\n",
    "    ignore_missing_vars: if False and there are variables in the model but not in the checkpoint, an error will be raised.\n",
    "    checkpoint_model_scope and model_name: if the root scope of checkpoints and the model in session is different,\n",
    "            (but the sub-scopes are all the same), specify them clearly\n",
    "    checkpoint_exclude_scopes: variables to be excluded when restoring from checkpoint_path.\n",
    "    Returns:\n",
    "      An init function run by the supervisor.\n",
    "    \"\"\"\n",
    "    import util\n",
    "    if util.str.is_none_or_empty(checkpoint_path):\n",
    "        return None\n",
    "    # Warn the user if a checkpoint exists in the train_dir. Then ignore.\n",
    "    if tf.train.latest_checkpoint(train_dir):\n",
    "        tf.logging.info(\n",
    "            'Ignoring --checkpoint_path because a checkpoint already exists in %s'\n",
    "            % train_dir)\n",
    "        return None\n",
    "\n",
    "    exclusions = []\n",
    "    if checkpoint_exclude_scopes:\n",
    "        exclusions = [scope.strip()\n",
    "                      for scope in checkpoint_exclude_scopes.split(',')]\n",
    "\n",
    "    # TODO(sguada) variables.filter_variables()\n",
    "    variables_to_restore = []\n",
    "    for var in slim.get_model_variables():\n",
    "        excluded = False\n",
    "        for exclusion in exclusions:\n",
    "            if var.op.name.startswith(exclusion):\n",
    "                excluded = True\n",
    "                break\n",
    "        if not excluded:\n",
    "            variables_to_restore.append(var)\n",
    "    # Change model scope if necessary.\n",
    "    if checkpoint_model_scope is not None:\n",
    "        variables_to_restore = {checkpoint_model_scope + '/' + var.op.name : var for var in variables_to_restore}\n",
    "        tf.logging.info('variables_to_restore: %r'%(variables_to_restore))\n",
    "    checkpoint_path = get_latest_ckpt(checkpoint_path)\n",
    "    tf.logging.info('Fine-tuning from %s. Ignoring missing vars: %s' % (checkpoint_path, ignore_missing_vars))\n",
    "    return slim.assign_from_checkpoint_fn(\n",
    "        checkpoint_path,\n",
    "        variables_to_restore,\n",
    "        ignore_missing_vars=ignore_missing_vars)\n",
    "\n",
    "\n",
    "def get_variables_to_train(flags = None):\n",
    "    \"\"\"code from github/SSD-tensorflow/tf_utils.py\n",
    "    Returns a list of variables to train.\n",
    "\n",
    "    Returns:\n",
    "      A list of variables to train by the optimizer.\n",
    "    \"\"\"\n",
    "    if flags is None or flags.trainable_scopes is None:\n",
    "        return tf.trainable_variables()\n",
    "    else:\n",
    "        scopes = [scope.strip() for scope in flags.trainable_scopes.split(',')]\n",
    "\n",
    "    variables_to_train = []\n",
    "    for scope in scopes:\n",
    "        variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope)\n",
    "        variables_to_train.extend(variables)\n",
    "    return variables_to_train\n",
    "\n",
    "def Print(tensor, data, msg = '', file = None, mode = 'w'):\n",
    "    from tensorflow.python.ops import control_flow_ops\n",
    "    import util\n",
    "    def np_print(*args):\n",
    "        if util.str.contains(msg, '%'):\n",
    "            message = msg%tuple(args)\n",
    "        else:\n",
    "            message = msg + ' %'*len(args)%tuple(args)\n",
    "        if file is not None:\n",
    "            file_path = util.io.get_absolute_path(file)\n",
    "            print('writting message to file(%s):'%(file_path), message)\n",
    "            with open(file_path, mode) as f:\n",
    "                print(message, file = f)\n",
    "        else:\n",
    "            print(message)\n",
    "    return control_flow_ops.with_dependencies([tf.py_func(np_print, data, [])], tensor)\n",
    "\n",
    "def get_variable_names_in_checkpoint(path, return_shapes = False, return_reader = False):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        path: the path to training directory containing checkpoints,\n",
    "            or path to checkpoint\n",
    "    Return:\n",
    "        a list of variable names in the checkpoint\n",
    "    \"\"\"\n",
    "    import util\n",
    "    ckpt = get_latest_ckpt(path)\n",
    "    ckpt_reader = tf.train.NewCheckpointReader(ckpt)\n",
    "    ckpt_vars = ckpt_reader.get_variable_to_shape_map()\n",
    "    names = [var for var in ckpt_vars]\n",
    "    if return_shapes:\n",
    "        return names, ckpt_vars\n",
    "    def get(name):\n",
    "        return ckpt_reader.get_tensor(name)\n",
    "    if return_reader:\n",
    "        return names, get\n",
    "    return names\n",
    "\n",
    "\n",
    "\n",
    "def min_area_rect(xs, ys):\n",
    "    import util\n",
    "    rects = tf.py_func(util.img.min_area_rect, [xs, ys], xs.dtype)\n",
    "    rects.set_shape([None, 5])\n",
    "    return rects\n",
    "\n",
    "\n",
    "def gpu_config(config = None, allow_growth = None, gpu_memory_fraction = None):\n",
    "    if config is None:\n",
    "        config = tf.ConfigProto()\n",
    "\n",
    "    if allow_growth is not None:\n",
    "        config.gpu_options.allow_growth = allow_growth\n",
    "\n",
    "    if gpu_memory_fraction is not None:\n",
    "        config.gpu_options.per_process_gpu_memory_fraction = gpu_memory_fraction\n",
    "\n",
    "    return config\n",
    "\n",
    "def wait_for_checkpoint(path):\n",
    "    from tensorflow.contrib.training.python.training import evaluation\n",
    "    return evaluation.checkpoints_iterator(path)\n",
    "    \n",
    "def focal_loss(labels, logits, gamma = 2.0, alpha = 0.75, normalize = True):\n",
    "    labels = tf.where(labels > 0, tf.ones_like(labels), tf.zeros_like(labels))\n",
    "    labels = tf.cast(labels, tf.float32)\n",
    "    probs = tf.sigmoid(logits)\n",
    "    CE = tf.nn.sigmoid_cross_entropy_with_logits(labels = labels, logits = logits)\n",
    "\n",
    "    alpha_t = tf.ones_like(logits) * alpha\n",
    "    alpha_t = tf.where(labels > 0, alpha_t, 1.0 - alpha_t)\n",
    "    probs_t = tf.where(labels > 0, probs, 1.0 - probs)\n",
    "\n",
    "    focal_matrix = alpha_t * tf.pow((1.0 - probs_t), gamma)\n",
    "    fl = focal_matrix * CE\n",
    "\n",
    "    fl = tf.reduce_sum(fl)\n",
    "    if normalize:\n",
    "        #n_pos = tf.reduce_sum(labels)\n",
    "        #fl = fl / tf.cast(n_pos, tf.float32)\n",
    "        total_weights = tf.stop_gradient(tf.reduce_sum(focal_matrix))\n",
    "        fl = fl / total_weights\n",
    "    return fl\n",
    "\n",
    "\n",
    "def focal_loss_layer_initializer(sigma = 0.01, pi = 0.01):\n",
    "    import numpy as np\n",
    "    b0 = - np.log((1 - pi) / pi)\n",
    "    return tf.random_normal_initializer(stddev = sigma), \\\n",
    "            tf.constant_initializer(b0)\n",
    "\n",
    "\n",
    "def sum_gradients(clone_grads, do_summary = False):                        \n",
    "    averaged_grads = []\n",
    "    for grad_and_vars in zip(*clone_grads):\n",
    "        grads = []\n",
    "        var = grad_and_vars[0][1]\n",
    "        try:\n",
    "            for g, v in grad_and_vars:\n",
    "                assert v == var\n",
    "                grads.append(g)\n",
    "            grad = tf.add_n(grads, name = v.op.name + '_summed_gradients')\n",
    "        except:\n",
    "            import pdb\n",
    "            pdb.set_trace()\n",
    "        \n",
    "        averaged_grads.append((grad, v))\n",
    "        \n",
    "        if do_summary:\n",
    "            tf.summary.histogram(\"variables_and_gradients_\" + grad.op.name, grad)\n",
    "            tf.summary.histogram(\"variables_and_gradients_\" + v.op.name, v)\n",
    "            tf.summary.scalar(\"variables_and_gradients_\" + grad.op.name+\\\n",
    "                  '_mean/var_mean', tf.reduce_mean(grad)/tf.reduce_mean(var))\n",
    "            tf.summary.scalar(\"variables_and_gradients_\" + v.op.name+'_mean',tf.reduce_mean(var))\n",
    "    return averaged_grads\n",
    "\n",
    "def get_update_op():\n",
    "    \"\"\"\n",
    "    Extremely important for BatchNorm\n",
    "    \"\"\"\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    if update_ops:\n",
    "        return tf.group(*update_ops)\n",
    "    return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#coding=utf-8\n",
    "\n",
    "import logging\n",
    "\n",
    "def add_to_path(path):\n",
    "    '''\n",
    "    add path to sys.path.\n",
    "    '''\n",
    "    import sys;\n",
    "    sys.path.insert(0, path);\n",
    "\n",
    "def add_ancester_dir_to_path(fp, p):\n",
    "    '''\n",
    "    add ancester directory to sys.path.\n",
    "    fp: usually __file__\n",
    "    p : the relative path to be added.\n",
    "    '''\n",
    "    import util\n",
    "    parent_path = util.io.get_dir(fp)\n",
    "    path = util.io.join_path(parent_path, p)\n",
    "    add_to_path(path)\n",
    "\n",
    "def is_main(mod_name):\n",
    "    return mod_name == '__main__'\n",
    "    \n",
    "def import_by_name(mod_name):\n",
    "    __import__(mod_name)\n",
    "    return get_mod_by_name(mod_name)\n",
    "\n",
    "def try_import_by_name(mod_name, error_path):\n",
    "    try:\n",
    "        import_by_name(mod_name)\n",
    "    except ImportError:\n",
    "        logging.info('adding %s to sys.path'%(error_path))\n",
    "        add_to_path(error_path)        \n",
    "        import_by_name(mod_name)\n",
    "    \n",
    "    return get_mod_by_name(mod_name)\n",
    "    \n",
    "def get_mod_by_name(mod_name):\n",
    "    import sys\n",
    "    return sys.modules[mod_name]\n",
    "    \n",
    "def load_mod_from_path(path, keep_name = True):\n",
    "    \"\"\"\"\n",
    "    Params:\n",
    "        path\n",
    "        keep_name: if True, the filename will be used as module name.\n",
    "    \"\"\"\n",
    "    import util\n",
    "    import imp\n",
    "    path = util.io.get_absolute_path(path)\n",
    "    file_name = util.io.get_filename(path)\n",
    "    module_name = file_name.split('.')[0]\n",
    "    if not keep_name:\n",
    "        module_name = '%s_%d'%(module_name, util.get_count())\n",
    "    return imp.load_source(module_name, path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available, with CUDA installed\n",
      "GPU is available, with CUDA installed\n",
      "GPU is available, with CUDA installed\n",
      "GPU is available, with CUDA installed\n",
      "/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "CUDA_VISIBLE_DEVICES=1,2\n",
    "\n",
    "def test_is_gpu_available():\n",
    "    for i in range(4):\n",
    "        if is_gpu_available():\n",
    "          print( \"GPU is available, %s CUDA installed\"%('with' if is_gpu_available(True) else 'without'))\n",
    "\n",
    "def test_get_available_gpus():\n",
    "    devices =get_available_gpus();\n",
    "    for d in devices:\n",
    "        print (d)\n",
    "        \n",
    "if is_main(__name__):\n",
    "    test_is_gpu_available()\n",
    "    test_get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
